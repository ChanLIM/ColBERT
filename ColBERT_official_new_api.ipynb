{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MG2OOdvDBH9w"
      },
      "source": [
        "# ColBERT: Indexing & Search Notebook v0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54YyOaE6BH9z"
      },
      "source": [
        "We start by importing the relevant classes. As we'll see below, `Indexer` and `Searcher` are the key actors here. "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone -b new_api https://github.com/stanford-futuredata/ColBERT.git"
      ],
      "metadata": {
        "id": "yuss_rCQLQ57",
        "outputId": "d40c93d7-c577-4f34-e260-0022b94646ae",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ColBERT'...\n",
            "remote: Enumerating objects: 516, done.\u001b[K\n",
            "remote: Counting objects: 100% (356/356), done.\u001b[K\n",
            "remote: Compressing objects: 100% (264/264), done.\u001b[K\n",
            "remote: Total 516 (delta 169), reused 220 (delta 87), pack-reused 160\u001b[K\n",
            "Receiving objects: 100% (516/516), 255.34 KiB | 3.99 MiB/s, done.\n",
            "Resolving deltas: 100% (224/224), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd ColBERT"
      ],
      "metadata": {
        "id": "5c02eyLRLy4R",
        "outputId": "47dd0d16-2c93-4354-b0b2-7f5497b29c77",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/ColBERT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get update\n",
        "!apt-get install git"
      ],
      "metadata": {
        "id": "bP799vp3Mmgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ujson\n",
        "!pip install GitPython\n",
        "!pip install transformers==3.0.2\n",
        "!pip install faiss-gpu==1.6.3"
      ],
      "metadata": {
        "id": "zhJjcuCJMT2u",
        "outputId": "acf2401f-48ed-457c-af15-040904c6c58f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ujson in /usr/local/lib/python3.7/dist-packages (5.1.0)\n",
            "Requirement already satisfied: GitPython in /usr/local/lib/python3.7/dist-packages (3.1.24)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from GitPython) (3.10.0.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.7/dist-packages (from GitPython) (4.0.9)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.7/dist-packages (from gitdb<5,>=4.0.1->GitPython) (5.0.0)\n",
            "Requirement already satisfied: transformers==3.0.2 in /usr/local/lib/python3.7/dist-packages (3.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2019.12.20)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (21.3)\n",
            "Requirement already satisfied: tokenizers==0.8.1.rc1 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.8.1rc1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (1.19.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (4.62.3)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.0.46)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92 in /usr/local/lib/python3.7/dist-packages (from transformers==3.0.2) (0.1.96)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==3.0.2) (3.0.6)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==3.0.2) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==3.0.2) (1.15.0)\n",
            "Collecting faiss-gpu==1.6.3\n",
            "  Downloading faiss_gpu-1.6.3-cp37-cp37m-manylinux2010_x86_64.whl (35.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 35.5 MB 265 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from faiss-gpu==1.6.3) (1.19.5)\n",
            "Installing collected packages: faiss-gpu\n",
            "Successfully installed faiss-gpu-1.6.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "C5bBjNufBH90"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "sys.path.insert(0, '../')\n",
        "\n",
        "from colbert.infra import Run, RunConfig\n",
        "from colbert.data import Queries, Collection\n",
        "from colbert import Indexer, Searcher"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tzCqwbYSBH91"
      },
      "source": [
        "The workflow here assumes an IR dataset: a set of queries and a corresponding collection of passages.\n",
        "\n",
        "The classes `Queries` and `Collection` provide a convenient interface for working with such datasets.\n",
        "\n",
        "We'll load the answer posts of the English Language Learners (ELL) StackExchange community as our collection, and use relevant GooAQ questions as our queries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "67EDkILQBH92",
        "outputId": "44011bd1-d927-4b99-8edb-b1fd037b141f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Dec 23, 02:52:41] #> Loading the queries from /future/u/okhattab/data/tmp/stackexchange/ell/questions.tsv ...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-ec9aec631056>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mcollection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataroot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'collection.answeronly.tsv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mqueries\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQueries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqueries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mcollection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCollection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ColBERT/colbert/data/queries.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path, data)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ColBERT/colbert/data/queries.py\u001b[0m in \u001b[0;36m_load_file\u001b[0;34m(self, path)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_load_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_queries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/ColBERT/colbert/evaluation/loaders.py\u001b[0m in \u001b[0;36mload_queries\u001b[0;34m(queries_path)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0mprint_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"#> Loading the queries from\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqueries_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqueries_path\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m             \u001b[0mqid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/future/u/okhattab/data/tmp/stackexchange/ell/questions.tsv'"
          ]
        }
      ],
      "source": [
        "dataroot = '/future/u/okhattab/data/tmp/stackexchange/'\n",
        "dataset = 'ell'\n",
        "\n",
        "queries = os.path.join(dataroot, dataset, 'questions.tsv')\n",
        "collection = os.path.join(dataroot, dataset, 'collection.answeronly.tsv')\n",
        "\n",
        "queries = Queries(path=queries)\n",
        "collection = Collection(path=collection)\n",
        "\n",
        "f'Loaded {len(queries)} queries and {len(collection):,} passages'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fLuirxU9BH93"
      },
      "source": [
        "This loaded 441 queries and 214,300 passages, numbered (in this dataset) from 0 onwathrough 440. Let's inspect one query and one passage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2T8r5USXBH93",
        "outputId": "989d33cb-466c-40b4-d004-24bc269acc84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "are disease names proper nouns?\n",
            "\n",
            "No, “chance” and “get a chance” do not mean the same thing. “Get a chance” means “have an opportunity”, but the verb “chance” alone doesn't. The verb “chance” has several meanings, but none of them work here. In the sense of something happening by luck, it's normally used in a past tense (“I never chanced to meet him” = “I was not lucky enough to meet him”) or hypothetically (“If you chance to meet him, say hello from me” = “If you luck into meeting him, say hello from me”). It doesn't make sense here since by definition chance implies that\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(queries[23])\n",
        "print()\n",
        "print(collection[79852])\n",
        "print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f55HaODSBH94"
      },
      "source": [
        "## Indexing\n",
        "\n",
        "For efficient search, we can pre-compute the ColBERT representation of each passage and index them.\n",
        "\n",
        "Below, the `Indexer` take a model checkpoint and writes a (compressed) index to disk. We then prepare a `Searcher` for retrieval from this index.\n",
        "\n",
        "(On future machines with four Titan V GPUs, indexing should take 6--7 minutes. The output is fairly ugly at the moment!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QOdxDtTtBH95",
        "outputId": "a5be558c-91f4-4b2b-e475-aa673c90718f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{\n",
            "    \"DocSettings\": {\n",
            "        \"dim\": 128,\n",
            "        \"doc_maxlen\": 220,\n",
            "        \"mask_punctuation\": true\n",
            "    },\n",
            "    \"IndexingSettings\": {\n",
            "        \"centroid_fraction_of_sample\": 0.03,\n",
            "        \"chunksize\": 2.0,\n",
            "        \"compression_level\": 1,\n",
            "        \"compression_thresholds\": \"\\/future\\/u\\/keshav2\\/compression_thresholds.csv\",\n",
            "        \"index_root\": null,\n",
            "        \"kmeans_niters\": 20,\n",
            "        \"kmeans_spherical\": true,\n",
            "        \"partitions\": null,\n",
            "        \"sample\": 0.05\n",
            "    },\n",
            "    \"QuerySettings\": {\n",
            "        \"query_maxlen\": 32\n",
            "    },\n",
            "    \"ResourceSettings\": {\n",
            "        \"checkpoint\": \"\\/dfs\\/scratch0\\/okhattab\\/OpenQA\\/colbert-400000.dnn\",\n",
            "        \"collection\": \"\\/future\\/u\\/okhattab\\/data\\/tmp\\/stackexchange\\/ell\\/collection.answeronly.tsv\",\n",
            "        \"index_name\": \"ell.index\",\n",
            "        \"queries\": null,\n",
            "        \"triples\": null\n",
            "    },\n",
            "    \"RunSettings\": {\n",
            "        \"amp\": true,\n",
            "        \"experiment\": \"notebook\",\n",
            "        \"gpus\": [\n",
            "            0,\n",
            "            1,\n",
            "            2,\n",
            "            3\n",
            "        ],\n",
            "        \"name\": \"2021-09-21\\/19.49.08\",\n",
            "        \"nranks\": 4,\n",
            "        \"overwrite\": false,\n",
            "        \"rank\": -1,\n",
            "        \"root\": \"\\/future\\/u\\/okhattab\\/repos\\/ColBERT-private-releases\\/experiments\"\n",
            "    },\n",
            "    \"SearchSettings\": {\n",
            "        \"faiss_depth\": 16384,\n",
            "        \"nprobe\": 2\n",
            "    },\n",
            "    \"TrainingSettings\": {\n",
            "        \"accumsteps\": 1,\n",
            "        \"bsize\": 64,\n",
            "        \"lr\": 3e-6,\n",
            "        \"maxsteps\": 500000,\n",
            "        \"resume\": false,\n",
            "        \"save_every\": null,\n",
            "        \"similarity\": \"cosine\"\n",
            "    }\n",
            "}\n",
            "\n",
            "\n",
            "[Sep 21, 19:49:08] #> Note: Output directory /future/u/okhattab/repos/ColBERT-private-releases/experiments/notebook/indexes/ell.index already exists\n",
            "\n",
            "\n",
            "#> Preparing...\n",
            "#> Preparing...\n",
            "#> Preparing...\n",
            "#> Preparing...\n",
            "[Sep 21, 19:49:09] [Pre-Emptying] GPU memory check: r=0, a=0, f=0\n",
            "[Sep 21, 19:49:09] [Post-Emptying] GPU memory check: r=0, a=0, f=0\n",
            "#> Starting...\n",
            "#> Starting...\n",
            "#> Starting...\n",
            "#> Starting...\n",
            "nranks = 4 \t num_gpus = 4 \t device=1\n",
            "[Sep 21, 19:50:07] [1] \t\t #> Encoding 18994 passages..\n",
            "nranks = 4 \t num_gpus = 4 \t device=3\n",
            "[Sep 21, 19:50:07] [3] \t\t #> Encoding 18983 passages..\n",
            "nranks = 4 \t num_gpus = 4 \t device=0\n",
            "[Sep 21, 19:50:07] [0] \t\t # of sampled PIDs = 81138 \t sampled_pids[:3] = [109214, 192069, 2665]\n",
            "nranks = 4 \t num_gpus = 4 \t device=2\n",
            "[Sep 21, 19:50:07] [2] \t\t #> Encoding 18845 passages..\n",
            "[Sep 21, 19:50:07] [0] \t\t #> Encoding 24316 passages..\n",
            "[Sep 21, 19:50:57] [0] \t\t avg_doclen_est = 78.97882843017578 \t len(local_sample) = 24,316\n",
            "[Sep 21, 19:50:57] [2] \t\t avg_doclen_est = 78.97882843017578 \t len(local_sample) = 18,845\n",
            "[Sep 21, 19:50:57] [3] \t\t avg_doclen_est = 78.97882843017578 \t len(local_sample) = 18,983\n",
            "[Sep 21, 19:50:57] [1] \t\t avg_doclen_est = 78.97882843017578 \t len(local_sample) = 18,994\n",
            "[Sep 21, 19:51:01] [0] \t\t Creaing 65,536 partitions. *Estimated* 16,925,162.93258667 embeddings.\n",
            "[Sep 21, 19:51:01] [0] \t\t #> Saving the indexing plan to /future/u/okhattab/repos/ColBERT-private-releases/experiments/notebook/indexes/ell.index/plan.json ..\n",
            "Clustering 6168239 points in 128D to 65536 clusters, redo 1 times, 20 iterations\n",
            "  Preprocessing in 0.65 s\n",
            "  Iteration 19 (138.63 s, search 133.16 s): objective=5.0599e+06 imbalance=1.333 nsplit=0        \n",
            "[0.041, 0.044, 0.043, 0.044, 0.046, 0.046, 0.044, 0.043, 0.043, 0.045, 0.045, 0.043, 0.041, 0.045, 0.043, 0.041, 0.044, 0.044, 0.044, 0.046, 0.043, 0.041, 0.047, 0.041, 0.046, 0.049, 0.044, 0.043, 0.043, 0.042, 0.043, 0.044, 0.045, 0.044, 0.044, 0.043, 0.039, 0.049, 0.042, 0.041, 0.043, 0.039, 0.043, 0.046, 0.043, 0.045, 0.045, 0.042, 0.043, 0.041, 0.044, 0.042, 0.042, 0.043, 0.042, 0.043, 0.04, 0.04, 0.041, 0.043, 0.042, 0.047, 0.044, 0.043, 0.05, 0.05, 0.046, 0.044, 0.043, 0.046, 0.04, 0.041, 0.043, 0.045, 0.045, 0.045, 0.044, 0.04, 0.044, 0.043, 0.044, 0.043, 0.041, 0.042, 0.043, 0.04, 0.041, 0.042, 0.044, 0.046, 0.041, 0.039, 0.046, 0.043, 0.043, 0.039, 0.041, 0.048, 0.042, 0.048, 0.042, 0.047, 0.044, 0.044, 0.042, 0.048, 0.042, 0.039, 0.04, 0.042, 0.044, 0.043, 0.043, 0.046, 0.044, 0.042, 0.041, 0.045, 0.042, 0.039, 0.043, 0.041, 0.039, 0.041, 0.045, 0.04, 0.042, 0.048]\n",
            "[Sep 21, 19:53:35] avg_residual = 0.043212890625\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "0it [00:00, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Sep 21, 19:53:36] [0] \t\t #> Encoding 25000 passages..\n",
            "[Sep 21, 19:53:36] [2] \t\t #> Encoding 25000 passages..\n",
            "[Sep 21, 19:53:36] [1] \t\t #> Encoding 25000 passages..\n",
            "[Sep 21, 19:53:36] [3] \t\t #> Encoding 25000 passages..\n",
            "[Sep 21, 19:54:24] [0] \t\t #> Saving chunk 0: \t 25,000 passages and 2,050,051 embeddings. From #0 onward.\n",
            "[Sep 21, 19:54:32] [1] \t\t #> Encoding 25000 passages..\n",
            "[Sep 21, 19:54:32] [3] \t\t #> Encoding 25000 passages..\n",
            "[Sep 21, 19:54:33] [2] \t\t #> Encoding 25000 passages..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1it [00:57, 57.25s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Sep 21, 19:54:33] [0] \t\t #> Encoding 25000 passages..\n",
            "[Sep 21, 19:55:21] [0] \t\t #> Saving chunk 4: \t 25,000 passages and 1,922,803 embeddings. From #100,000 onward.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2it [01:52, 56.37s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Sep 21, 19:55:29] [0] \t\t #> Encoding 14300 passages..\n",
            "[Sep 21, 19:55:56] [0] \t\t #> Saving chunk 8: \t 14,300 passages and 1,225,125 embeddings. From #200,000 onward.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "3it [02:25, 48.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Sep 21, 19:56:10] [0] \t\t #> Saving the indexing metadata to /future/u/okhattab/repos/ColBERT-private-releases/experiments/notebook/indexes/ell.index/metadata.json ..\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#> Joined...\n",
            "#> Joined...\n",
            "#> Joined...\n",
            "#> Joined...\n",
            "[Sep 21, 19:56:13] #> Loading collection...\n",
            "0M \n",
            "#> WARNING: StridedTensor has to add padding, internally, to a large tensor.\n",
            "#> WARNING: Consider doing this padding in advance to save memory!\n",
            "[Sep 21, 19:56:27] #> Building the emb2pid mapping..\n",
            "[Sep 21, 19:56:28] len(self.emb2pid) = 16976942\n"
          ]
        }
      ],
      "source": [
        "with Run().context(RunConfig(nranks=4, experiment='notebook')):  # nranks specifies the number of GPUs to use.\n",
        "    index_name = f'{dataset}.index'\n",
        "\n",
        "    indexer = Indexer(checkpoint='/dfs/scratch0/okhattab/OpenQA/colbert-400000.dnn')  # MS MARCO ColBERT checkpoint\n",
        "    indexer.index(name=index_name, collection=collection, overwrite=True)\n",
        "\n",
        "    searcher = Searcher(index=index_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MKFJiyrsBH96"
      },
      "source": [
        "## Search\n",
        "\n",
        "Having built the index and prepared our `searcher`, we can search for individual query strings.\n",
        "\n",
        "We can use the `queries` set we loaded earlier — or you can supply your own questions, say, \"how large is the vocabulary of the average person?\"\n",
        "\n",
        "Feel free to get creative! But keep in mind this set of ~200k ELL passages can only answer a small, focused set of questions on English Language Learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zd9Xv5O6BH96",
        "outputId": "87424d0c-6603-41c6-9893-970714a117e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "#> are morphology and structure the same?\n",
            "\t [1] \t\t 21.3 \t\t This is tricky one, I'll try and explain my perspective as a native British English speaker. I have a biology background as opposed to physics, so that might come into play as well here. Some defintions: Structure: Arrangement of and relations between the parts or elements of something complex or a piece of construction. Whereas morphology: Morphology: A particular form, shape, or structure or the study of something's form of shape. As you can see, they both have one usage where they are near enough synonymous, and another where they are not. In most situations they can be\n",
            "\t [2] \t\t 19.6 \t\t Since each program has one structure, the correct sentence is Program A and program B have the same structure: they both have a sequential structure. There is a single structure, which is shared by the two programs. The “same structure” in the first sentence is the sequential structure, there is only one. Other ways to formulate this idea include The structure of program A is the same as the structure of program B. Both program A and program B have a sequential structure. The structures of program A and program B are the same. In this\n",
            "\t [3] \t\t 19.4 \t\t The structure is of the form of X. The structure is in the form of X. What is the difference in meaning and which one is grammatically correct? I think 1 means the structure is united with the form of X and 2 means the structure is shaped in the form of X So, as for 1, I don't think the entire shape of the structure must look in the form of X (maybe ,part of it is united with the form of X?), but as for 2, I do think the entire shape of the\n"
          ]
        }
      ],
      "source": [
        "query = queries[30]   # or supply your own query\n",
        "\n",
        "print(f\"#> {query}\")\n",
        "\n",
        "# Find the top-3 passages for this query\n",
        "results = searcher.search(query, k=3)\n",
        "\n",
        "# Print out the top-k retrieved passages\n",
        "for passage_id, passage_rank, passage_score in zip(*results):\n",
        "    print(f\"\\t [{passage_rank}] \\t\\t {passage_score:.1f} \\t\\t {searcher.collection[passage_id]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAsI2fu5BH97"
      },
      "source": [
        "## Batch Search\n",
        "\n",
        "In many applications, you have a large batch of queries and you need to maximize the overall throughput. For that, you can use the `searcher.search_all(queries, k)` method, which returns a `Ranking` object that organizes the results across all queries.\n",
        "\n",
        "(Batching provides many opportunities for higher-throughput search, though we have not implemented most of those optimizations for compressed indexes yet.)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "tags": [],
        "id": "5UpDiyAOBH97"
      },
      "outputs": [],
      "source": [
        "rankings = searcher.search_all(queries, k=5).todict()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ydzRfJIcBH98",
        "outputId": "d595e4bb-914c-4c07-dbfe-0d385255c8f4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[(54519, 1, 21.296875),\n",
              " (9029, 2, 19.625),\n",
              " (177893, 3, 19.4375),\n",
              " (156910, 4, 19.40625),\n",
              " (10821, 5, 19.34375)]"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rankings[30]  # For query 30, a list of (passage_id, rank, score) for the top-k passages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lm2J3UIPBH98"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GJ4XTYpjBH98"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "interpreter": {
      "hash": "a99ac6d2deb03d0b7ced3594556c328848678d7cea021ae1b9990e15d3ad5c49"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "name": "intro.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}